import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.cluster import KMeans

file_path = '/content/data.csv'  # Replace with the path to your data file
try:
    data_sample = pd.read_csv(file_path, encoding='utf-8')
except UnicodeDecodeError:
    data_sample = pd.read_csv(file_path, encoding='ISO-8859-1')

data_sample['InvoiceDate'] = pd.to_datetime(data_sample['InvoiceDate'])
current_date = data_sample['InvoiceDate'].max() + pd.to_timedelta(1, unit='d')


data_sample['TotalPrice'] = data_sample['Quantity'] * data_sample['UnitPrice']

customer_data = data_sample.groupby('CustomerID').agg({
    'InvoiceDate': lambda x: (current_date - x.max()).days,  # Recency
    'InvoiceNo': 'nunique',   # Frequency
    'TotalPrice': 'sum'       # Monetary value
})


customer_data.rename(columns={
    'InvoiceDate': 'Recency',
    'InvoiceNo': 'Frequency',
    'TotalPrice': 'MonetaryValue'
}, inplace=True)


scaler = StandardScaler()
scaled_customer_data = scaler.fit_transform(customer_data)


input_dim = scaled_customer_data.shape[1]

autoencoder = models.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu', name="encoded"),
    layers.Dense(64, activation='relu'),
    layers.Dense(input_dim, activation='linear')
])

autoencoder.compile(optimizer='adam', loss='mean_squared_error')


optimal_epoch = 93  
autoencoder.fit(scaled_customer_data, scaled_customer_data, epochs=optimal_epoch, batch_size=32, shuffle=True)


encoder = models.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer("encoded").output)


encoded_data = encoder.predict(scaled_customer_data)


wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(encoded_data)
    wcss.append(kmeans.inertia_)


k_values = range(1, 11)
optimal_k = np.argmax(np.diff(wcss)) + 2  # Plus 2 because index starts at 0 and we are interested in the point after the largest difference


kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=0)
clusters_kmeans = kmeans.fit_predict(encoded_data)

customer_data['Autoencoder_Cluster'] = clusters_kmeans


print(f'Number of Clusters: {optimal_k}')

cluster_counts = customer_data['Autoencoder_Cluster'].value_counts().reset_index()
cluster_counts.columns = ['Cluster', 'Number of Customers']
print(cluster_counts)

print(customer_data.head())

output_file_path = '/content/clustered_data_autoencoder.xlsx'
customer_data.to_excel(output_file_path, index=False)
